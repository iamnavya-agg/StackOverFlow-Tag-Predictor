{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DecisionTree.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/StackOverFlow-Tag-Predictor/blob/master/DecisionTree.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LW6ONJ6u11A"
      },
      "source": [
        "# Import\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vLhhHQr4uxmI",
        "outputId": "23ed3d6d-e1dd-4a29-a445-b08af97435c8"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import csv,nltk,random,pickle\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer,SnowballStemmer\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from wordcloud import WordCloud\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split,ShuffleSplit,GridSearchCV\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "import pickle\r\n",
        "import joblib\r\n",
        "import warnings\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUPJhS8ivAH0"
      },
      "source": [
        "# Decision Tree"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C1-Tm6D3u9Lx"
      },
      "source": [
        "def DecisionTree(n,preprocessed_df,yx_multilabel,grid=\"0\"):\r\n",
        "    size=int(0.8*n)\r\n",
        "    X_train=preprocessed_df.head(size)\r\n",
        "    vectorizer = CountVectorizer(min_df=0.0009, max_features=13000,tokenizer = lambda x: x.split(), ngram_range=(1,4))\r\n",
        "    vectorizer.fit(preprocessed_df['question'])\r\n",
        "    X_test=preprocessed_df.tail(preprocessed_df.shape[0] - size)\r\n",
        "    Y_train = yx_multilabel[0:size,:]\r\n",
        "    Y_test = yx_multilabel[size:preprocessed_df.shape[0],:]\r\n",
        "    x_train_multilabel = vectorizer.transform(X_train['question'])\r\n",
        "    x_test_multilabel = vectorizer.transform(X_test['question'])\r\n",
        "\r\n",
        "    if (grid==\"0\"):\r\n",
        "        clf= OneVsRestClassifier(DecisionTreeClassifier())\r\n",
        "    else:\r\n",
        "        model_to_set  = OneVsRestClassifier(DecisionTreeClassifier())\r\n",
        "        parameters= {\"estimator__max_depth\": [i for i in range(400,500,20)]}\r\n",
        "        clf=GridSearchCV(model_to_set, param_grid=parameters)\r\n",
        "\r\n",
        "    clf.fit(x_train_multilabel, Y_train)\r\n",
        "    pickle.dump(clf,open(\"DT_\"+ str(n) ,'wb'))\r\n",
        "    !cp /content/DT_\"$n\" /content/drive/MyDrive/MlProject\r\n",
        "    y_pred = clf.predict(x_test_multilabel)\r\n",
        "    print(\"Accuracy :\",metrics.accuracy_score(Y_test,y_pred))\r\n",
        "    print(\"Macro f1 score :\",metrics.f1_score(Y_test, y_pred, average = 'macro'))\r\n",
        "    print(\"Micro f1 scoore :\",metrics.f1_score(Y_test, y_pred, average = 'micro'))\r\n",
        "    print(\"hamming loss :\",metrics.hamming_loss(Y_test,y_pred))\r\n",
        "    # print(\"1 match accuracy :\", match_accuracy(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5wxMUbVfvq0X",
        "outputId": "a1a2b73d-810d-4846-c963-c41449042988"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "for i in range(10000,100001,10000):\r\n",
        "    print(\"Dataset size is \",i)\r\n",
        "    preprocessed_df=pd.read_csv(\"/content/drive/MyDrive/MlProject/data/vectorised_X_\"+str(i)+\"k.csv\")\r\n",
        "    yx_multilabel=joblib.load(\"/content/drive/MyDrive/MlProject/data/vectorised_y_\"+str(i)+\".pkl\")\r\n",
        "    DecisionTree(i,preprocessed_df,yx_multilabel)   \r\n",
        "    print(\"----------------------------------------------\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size is  10000\n",
            "Accuracy : 0.3496993987975952\n",
            "Macro f1 score : 0.38758200184970854\n",
            "Micro f1 scoore : 0.42823405930202046\n",
            "hamming loss : 0.02183366733466934\n",
            "----------------------------------------------\n",
            "Dataset size is  20000\n",
            "Accuracy : 0.37503139914594324\n",
            "Macro f1 score : 0.3970783185073287\n",
            "Micro f1 scoore : 0.44972028926183655\n",
            "hamming loss : 0.020261240894247676\n",
            "----------------------------------------------\n",
            "Dataset size is  30000\n",
            "Accuracy : 0.3788235294117647\n",
            "Macro f1 score : 0.3864280488232697\n",
            "Micro f1 scoore : 0.4489906762016837\n",
            "hamming loss : 0.020460504201680673\n",
            "----------------------------------------------\n",
            "Dataset size is  40000\n",
            "Accuracy : 0.36855441530470034\n",
            "Macro f1 score : 0.3726859708036517\n",
            "Micro f1 scoore : 0.42683264177040114\n",
            "hamming loss : 0.021000886861776258\n",
            "----------------------------------------------\n",
            "Dataset size is  50000\n",
            "Accuracy : 0.3834976120312976\n",
            "Macro f1 score : 0.39249325761050385\n",
            "Micro f1 scoore : 0.4476268861454047\n",
            "hamming loss : 0.020459302916370287\n",
            "----------------------------------------------\n",
            "Dataset size is  60000\n",
            "Accuracy : 0.37416121634247856\n",
            "Macro f1 score : 0.3992651657520239\n",
            "Micro f1 scoore : 0.44896655564173926\n",
            "hamming loss : 0.02051643591268156\n",
            "----------------------------------------------\n",
            "Dataset size is  70000\n",
            "Accuracy : 0.37207943925233644\n",
            "Macro f1 score : 0.38691699231863524\n",
            "Micro f1 scoore : 0.4470442395599288\n",
            "hamming loss : 0.020844042056074766\n",
            "----------------------------------------------\n",
            "Dataset size is  80000\n",
            "Accuracy : 0.37430275052894785\n",
            "Macro f1 score : 0.4002486890308167\n",
            "Micro f1 scoore : 0.4477925243770314\n",
            "hamming loss : 0.02091427838686927\n",
            "----------------------------------------------\n",
            "Dataset size is  90000\n",
            "Accuracy : 0.37780698245814526\n",
            "Macro f1 score : 0.4074293228807528\n",
            "Micro f1 scoore : 0.456334451800127\n",
            "hamming loss : 0.02055311125078567\n",
            "----------------------------------------------\n",
            "Dataset size is  100000\n",
            "Accuracy : 0.3810358155114661\n",
            "Macro f1 score : 0.40953740420186635\n",
            "Micro f1 scoore : 0.4602403058438012\n",
            "hamming loss : 0.02037206905436743\n",
            "----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aplNYftxv6pJ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
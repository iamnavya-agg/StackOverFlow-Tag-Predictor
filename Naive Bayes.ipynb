{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Nayes.ipynb",
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/StackOverFlow-Tag-Predictor/blob/master/Naive%20Bayes.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e84UXnh50IB7"
      },
      "source": [
        "# Import"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a7hAXvii0GIT",
        "outputId": "728f7f1d-61b4-4fe0-c654-0518d9a1d12b"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import csv,nltk,random,pickle\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer,SnowballStemmer\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from wordcloud import WordCloud\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split,ShuffleSplit,GridSearchCV\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "import pickle\r\n",
        "import joblib\r\n",
        "import warnings\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvul98YZ0Rb9"
      },
      "source": [
        "# Bayes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sDjx_Dgs0NdA"
      },
      "source": [
        "def NB(n,preprocessed_df,yx_multilabel,grid=\"0\"):\r\n",
        "    size=int(0.8*n)\r\n",
        "    X_train=preprocessed_df.head(size)\r\n",
        "    vectorizer = CountVectorizer(min_df=0.0009, max_features=13000,tokenizer = lambda x: x.split(), ngram_range=(1,4))\r\n",
        "    vectorizer.fit(preprocessed_df['question'])\r\n",
        "    X_test=preprocessed_df.tail(preprocessed_df.shape[0] - size)\r\n",
        "    Y_train = yx_multilabel[0:size,:]\r\n",
        "    Y_test = yx_multilabel[size:preprocessed_df.shape[0],:]\r\n",
        "    x_train_multilabel = vectorizer.transform(X_train['question'])\r\n",
        "    x_test_multilabel = vectorizer.transform(X_test['question'])\r\n",
        "\r\n",
        "    clf = OneVsRestClassifier(BernoulliNB())\r\n",
        "\r\n",
        "    clf.fit(x_train_multilabel, Y_train)\r\n",
        "    pickle.dump(clf,open(\"NB_\"+ str(n) ,'wb'))\r\n",
        "    # !cp /content/NB_\"$n\" /content/drive/MyDrive/MlProject\r\n",
        "    y_pred = clf.predict(x_test_multilabel)\r\n",
        "    print(\"Accuracy :\",metrics.accuracy_score(Y_test,y_pred))\r\n",
        "    print(\"Macro f1 score :\",metrics.f1_score(Y_test, y_pred, average = 'macro'))\r\n",
        "    print(\"Micro f1 scoore :\",metrics.f1_score(Y_test, y_pred, average = 'micro'))\r\n",
        "    print(\"hamming loss :\",metrics.hamming_loss(Y_test,y_pred))\r\n",
        "    # print(\"1 match accuracy :\", match_accuracy(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lk9jdDoZ1HTi",
        "outputId": "e09317dc-2f63-46dd-aa13-8a2641cce78f"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "for i in range(10000,100001,10000):\r\n",
        "    print(\"Dataset size is \",i)\r\n",
        "    preprocessed_df=pd.read_csv(\"/content/drive/MyDrive/MlProject/vectorised_X_\"+str(i)+\"k.csv\")\r\n",
        "    yx_multilabel=joblib.load(\"/content/drive/MyDrive/MlProject/vectorised_y_\"+str(i)+\".pkl\")\r\n",
        "    NB(i,preprocessed_df,yx_multilabel)   \r\n",
        "    print(\"----------------------------------------------\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size is  10000\n",
            "Accuracy : 0.3061122244488978\n",
            "Macro f1 score : 0.12341274590832278\n",
            "Micro f1 scoore : 0.24357490159759204\n",
            "hamming loss : 0.03273547094188377\n",
            "----------------------------------------------\n",
            "Dataset size is  20000\n",
            "Accuracy : 0.3054508917357448\n",
            "Macro f1 score : 0.19671764951385243\n",
            "Micro f1 scoore : 0.31000000000000005\n",
            "hamming loss : 0.037437829691032404\n",
            "----------------------------------------------\n",
            "Dataset size is  30000\n",
            "Accuracy : 0.2842016806722689\n",
            "Macro f1 score : 0.23313497697360533\n",
            "Micro f1 scoore : 0.33045690895010515\n",
            "hamming loss : 0.04068571428571428\n",
            "----------------------------------------------\n",
            "Dataset size is  40000\n",
            "Accuracy : 0.2665653110350944\n",
            "Macro f1 score : 0.25137999085110146\n",
            "Micro f1 scoore : 0.3282356162240388\n",
            "hamming loss : 0.0435195743063474\n",
            "----------------------------------------------\n",
            "Dataset size is  50000\n",
            "Accuracy : 0.2601361650238797\n",
            "Macro f1 score : 0.2711466514898928\n",
            "Micro f1 scoore : 0.3395846891084423\n",
            "hamming loss : 0.04537343765877452\n",
            "----------------------------------------------\n",
            "Dataset size is  60000\n",
            "Accuracy : 0.24114499278009002\n",
            "Macro f1 score : 0.2881366911160645\n",
            "Micro f1 scoore : 0.34981802611860413\n",
            "hamming loss : 0.04643336447804298\n",
            "----------------------------------------------\n",
            "Dataset size is  70000\n",
            "Accuracy : 0.22941004672897197\n",
            "Macro f1 score : 0.2955858574654696\n",
            "Micro f1 scoore : 0.3579555863135826\n",
            "hamming loss : 0.0460616238317757\n",
            "----------------------------------------------\n",
            "Dataset size is  80000\n",
            "Accuracy : 0.23203180098736936\n",
            "Macro f1 score : 0.3026971319337517\n",
            "Micro f1 scoore : 0.354010897402843\n",
            "hamming loss : 0.048192601141245114\n",
            "----------------------------------------------\n",
            "Dataset size is  90000\n",
            "Accuracy : 0.20855951088509228\n",
            "Macro f1 score : 0.31425691559139063\n",
            "Micro f1 scoore : 0.36353640098352147\n",
            "hamming loss : 0.04762584995143135\n",
            "----------------------------------------------\n",
            "Dataset size is  100000\n",
            "Accuracy : 0.2209224426694151\n",
            "Macro f1 score : 0.3126085798674633\n",
            "Micro f1 scoore : 0.3609191360445773\n",
            "hamming loss : 0.048701880958515846\n",
            "----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYLE7Jb72PZa"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
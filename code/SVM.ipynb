{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "SVM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/itissandeep98/StackOverFlow-Tag-Predictor/blob/master/SVM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ihmplB1IzmZ"
      },
      "source": [
        "# Imports\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g-YZb_sKIq1X",
        "outputId": "188a16a2-2921-4b04-d3c9-a5531f4dee78"
      },
      "source": [
        "import numpy as np\r\n",
        "import pandas as pd \r\n",
        "import csv,nltk,random,pickle\r\n",
        "from nltk.corpus import stopwords\r\n",
        "from nltk.tokenize import word_tokenize\r\n",
        "from nltk.stem import PorterStemmer,WordNetLemmatizer,SnowballStemmer\r\n",
        "import re\r\n",
        "import seaborn as sns\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "from wordcloud import WordCloud\r\n",
        "from sklearn import metrics\r\n",
        "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer\r\n",
        "from sklearn.model_selection import train_test_split,ShuffleSplit,GridSearchCV\r\n",
        "from sklearn.multiclass import OneVsRestClassifier\r\n",
        "from sklearn.linear_model import SGDClassifier\r\n",
        "from sklearn.preprocessing import LabelEncoder\r\n",
        "from sklearn.naive_bayes import GaussianNB,MultinomialNB,BernoulliNB\r\n",
        "from sklearn.ensemble import RandomForestClassifier\r\n",
        "import pickle\r\n",
        "import joblib\r\n",
        "import warnings\r\n",
        "from sklearn.model_selection import GridSearchCV\r\n",
        "from sklearn.tree import DecisionTreeClassifier\r\n",
        "nltk.download('stopwords')\r\n",
        "nltk.download('punkt')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "voqsPBeUKF4G"
      },
      "source": [
        "def match_accuracy(y,y_pred):\r\n",
        "    c= y==y_pred\r\n",
        "    c=np.sum(c,axis=1)\r\n",
        "    c[c >= 1] = 1\r\n",
        "    return np.sum(c)/len(c)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZhdXB9rbI5vu"
      },
      "source": [
        "# SVM\r\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9tPWRJxfI34Z"
      },
      "source": [
        "def SVM(n,preprocessed_df,yx_multilabel,grid=\"0\"):\r\n",
        "    size=int(0.8*n)\r\n",
        "    X_train=preprocessed_df.head(size)\r\n",
        "    vectorizer = CountVectorizer(min_df=0.0009, max_features=13000,tokenizer = lambda x: x.split(), ngram_range=(1,4))\r\n",
        "    vectorizer.fit(preprocessed_df['question'])\r\n",
        "    X_test=preprocessed_df.tail(preprocessed_df.shape[0] - size)\r\n",
        "    Y_train = yx_multilabel[0:size,:]\r\n",
        "    Y_test = yx_multilabel[size:preprocessed_df.shape[0],:]\r\n",
        "    x_train_multilabel = vectorizer.transform(X_train['question'])\r\n",
        "    x_test_multilabel = vectorizer.transform(X_test['question'])\r\n",
        "\r\n",
        "    if (grid==\"0\"):\r\n",
        "        clf = OneVsRestClassifier(SGDClassifier(alpha=0.001,penalty='l2'))\r\n",
        "    else:\r\n",
        "        model_to_set = OneVsRestClassifier(SGDClassifier(penalty='l2'))\r\n",
        "        parameters= {\"estimator__alpha\": [1/10**(-i) for i in range(1,10)]}\r\n",
        "        clf=GridSearchCV(model_to_set, param_grid=parameters)\r\n",
        "\r\n",
        "    clf.fit(x_train_multilabel, Y_train)\r\n",
        "    pickle.dump(clf,open(\"svm_\"+ str(n) ,'wb'))\r\n",
        "    # !cp /content/svm_\"$n\" /content/drive/MyDrive/MlProject/\r\n",
        "    y_pred = clf.predict(x_test_multilabel)\r\n",
        "    print(\"Accuracy :\",metrics.accuracy_score(Y_test,y_pred))\r\n",
        "    print(\"Macro f1 score :\",metrics.f1_score(Y_test, y_pred, average = 'macro'))\r\n",
        "    print(\"Micro f1 scoore :\",metrics.f1_score(Y_test, y_pred, average = 'micro'))\r\n",
        "    print(\"hamming loss :\",metrics.hamming_loss(Y_test,y_pred))\r\n",
        "    # print(\"1 match accuracy :\", match_accuracy(Y_test,y_pred))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rgGLW7zTI4ho",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "75258ffc-2907-4179-bb72-5f4d5b75dfb8"
      },
      "source": [
        "warnings.filterwarnings(\"ignore\")\r\n",
        "\r\n",
        "for i in range(10000,100001,10000):\r\n",
        "    print(\"Dataset size is \",i)\r\n",
        "    preprocessed_df=pd.read_csv(\"/content/drive/MyDrive/MlProject/data/vectorised_X_\"+str(i)+\"k.csv\")\r\n",
        "    yx_multilabel=joblib.load(\"/content/drive/MyDrive/MlProject/data/vectorised_y_\"+str(i)+\".pkl\")\r\n",
        "    SVM(i,preprocessed_df,yx_multilabel)   \r\n",
        "    print(\"----------------------------------------------\") "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dataset size is  10000\n",
            "Accuracy : 0.3807615230460922\n",
            "Macro f1 score : 0.3986084438327051\n",
            "Micro f1 scoore : 0.4431575739103931\n",
            "hamming loss : 0.018306613226452904\n",
            "----------------------------------------------\n",
            "Dataset size is  20000\n",
            "Accuracy : 0.4483798040693293\n",
            "Macro f1 score : 0.40585598977368437\n",
            "Micro f1 scoore : 0.48367313111221605\n",
            "hamming loss : 0.015649334338106002\n",
            "----------------------------------------------\n",
            "Dataset size is  30000\n",
            "Accuracy : 0.45915966386554624\n",
            "Macro f1 score : 0.3986015293244122\n",
            "Micro f1 scoore : 0.48934010152284263\n",
            "hamming loss : 0.015216806722689076\n",
            "----------------------------------------------\n",
            "Dataset size is  40000\n",
            "Accuracy : 0.46800962878499935\n",
            "Macro f1 score : 0.38972557853098166\n",
            "Micro f1 scoore : 0.48082569623632\n",
            "hamming loss : 0.014785252755606233\n",
            "----------------------------------------------\n",
            "Dataset size is  50000\n",
            "Accuracy : 0.48267452494665175\n",
            "Macro f1 score : 0.3981323967266199\n",
            "Micro f1 scoore : 0.49551090209491233\n",
            "hamming loss : 0.014388781627883346\n",
            "----------------------------------------------\n",
            "Dataset size is  60000\n",
            "Accuracy : 0.48025142274696336\n",
            "Macro f1 score : 0.3981082193743128\n",
            "Micro f1 scoore : 0.498230460080379\n",
            "hamming loss : 0.014210481610464622\n",
            "----------------------------------------------\n",
            "Dataset size is  70000\n",
            "Accuracy : 0.4693341121495327\n",
            "Macro f1 score : 0.3700837562653732\n",
            "Micro f1 scoore : 0.4785327219674864\n",
            "hamming loss : 0.014614485981308411\n",
            "----------------------------------------------\n",
            "Dataset size is  80000\n",
            "Accuracy : 0.4727832275437584\n",
            "Macro f1 score : 0.3828802211497184\n",
            "Micro f1 scoore : 0.485474557921328\n",
            "hamming loss : 0.014625889594152721\n",
            "----------------------------------------------\n",
            "Dataset size is  90000\n",
            "Accuracy : 0.47797268727501285\n",
            "Macro f1 score : 0.3824580226025428\n",
            "Micro f1 scoore : 0.4900287205210145\n",
            "hamming loss : 0.014407176732758127\n",
            "----------------------------------------------\n",
            "Dataset size is  100000\n",
            "Accuracy : 0.478691059005411\n",
            "Macro f1 score : 0.3787387727168347\n",
            "Micro f1 scoore : 0.4911896683813068\n",
            "hamming loss : 0.014374645709868591\n",
            "----------------------------------------------\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MFXjaGDSQnHh"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}